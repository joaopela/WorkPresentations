\documentclass[8pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{epsfig}
% \usepackage{cancel}
\usepackage{ulem}
% \usepackage{threeparttable} % Joao Pela: 
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{siunitx}  % Allows easy x10^ for numbers
\usepackage{appendixnumberbeamer}

\usetheme{Madrid}

\author[J. Pela]{JoÃ£o Pela\\{\fontsize{5}{2}\selectfont on behalf of the Higgs to Invisible Analysis Group}}
\title{QCD VBF Jets+MET samples for Run 2}
\institute[ICL]{Imperial College London}
\date{2015-07-29}

% The log drawn in the upper right corner.
\logo{\includegraphics[height=0.115\paperheight]{img/Logo_CMSICL.png}}

\begin{document}
\setlength{\unitlength}{1mm}

% ###################################################
\begin{frame}
  \titlepage
\end{frame}

% Jim Points 
% - we have some problems with QCD MC resulting from our signal region : high mass jet pair with relatively modest pT and MET
% - our ability to study QCD BG estimation methods is hampered by lack of MC statistics
% - for parked analysis we generated a private QCD sample with GEN level filters, 
% - 
% - 
% - brief overview of the possible selections Joao has studied, and calculated the filter efficiencies
% - in order to make a cmsDriver command, we need changes to cmsDriver (to include post-L1 filter) and addition of the filter itself

% ###################################################
\begin{frame}{Introduction}
  
\begin{block}{Signal Topology: VBF Higgs to Invisible}
  
\begin{itemize}
  \item Two VBF jets with high mass but relatively low $p_\perp$ ($\sim 50$ GeV)
  \item Reasonable amount of MET
\end{itemize}
  
\end{block}

Our ability to study QCD BG estimation methods is hampered by lack of MC statistics

\begin{block}{QCD Private production during Run I}

During Run I a set of QCD samples with VBF like jets and real MET was generated with:

\begin{itemize}
  \item Real MET (sum os neutrino $p_\perp$)
  \item Two generator jets with high separation and mass
\end{itemize}

They allowed:

\begin{itemize}
  \item Understand real MET in QCD
  \item Indirectly ``determine the importance`` of fake MET events (which we were not modelling)
\end{itemize}
  
\end{block}

\begin{center}
This is not ideal because fake MET is important in our signal region
\end{center}

\end{frame}

% ###################################################
\begin{frame}{Proposal}

We propose to generate samples of QCD with:
\begin{block}{Filters}

\begin{itemize}
  \item Filter \#1: VBF jet selection at GEN level
  \item Filter \#2: MET applied after digitisation (implemented as a L1 MET cut)
\end{itemize}
  
\end{block}

By carefully tuning the cuts at each filter we could:
\begin{block}{Advantages:}

\begin{itemize}
  \item Produce a QCD sample with high equivalent integrated luminosity (several $fb^{-1}$)
  \item Generation cuts below trigger/offline analysis cuts 
  \item Able to simulate both real QCD MET and fake/mismeasured MET
  \item If well understood possibility of using for MVA training
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Assumptions and Questions}
  
\begin{block}{Assumptions}

For this study it was assumed reasonable for full production: 

\begin{itemize}
  \item 5000 CPU for up to 2-3 months (60 to 90 days)
  \begin{itemize}
    \item Is this a reasonable amount of computing power? If not which should be targeted? 
  \end{itemize}
  \item For an equivalent integrated luminosity of 10 $fb^-1$
  \begin{itemize}
    \item If the computing requirements resources are too high a smaller equivalent luminosity sample could be made. 
  \end{itemize}
  \item The sample final size should be comparable to current produced samples
\end{itemize}

\end{block}
  
\end{frame}

% ###################################################
\begin{frame}{Expected QCD events}

In order to determine which $p_\perp$ hats we would need to generate we calculate what are Phys14 QCD MC equivalent luminosities.

\begin{block}{Equivalent luminosity per $p_\perp$ hat for Phys14 samples}

\input{tables/table_QCD_Phys14_EquivalentLumi.tex}

\end{block}

\begin{block}{Minimum $p_\perp$ hat to simulate}

\begin{itemize}
  \item For 10 $fb^-1$ we need to simulate up to bin 470-600
  \item For 30 $fb^-1$ we need to simulate up to bin 800-1000
  \item Above this $p_\perp$ hats we can use the QCD inclusive samples since they have enough equivalent luminosity.
\end{itemize}

\end{block}

\end{frame}

% ###################################################
\begin{frame}{Generator filter working points}

The generator level filter will select events with at least a pair of generator jets (ak4) passing a set of cuts.

\begin{block}{Generator Level Filter Statistics}

  \centering
  \input{tbl/table_GenFilter_Summary2}
  
\end{block}

\begin{itemize}
  \item All times are HS06 normalized (to reduce extrapolation error)
  \item Working Point A: Lowering $p_\perp$ even by 5 GeV increases processing time prohibitively 
  \item Working Point A: Lowering $\Delta\eta$ cut makes almost no differences
  \item Working Point A: For each 100 GeV drop of $m_{jj}$ will cost of at least 20 additional days. 
  \item Increasing $min(\Delta\phi)$ to 2.0 and decreasing $\Delta\eta$ to 3.0 is acceptable (Working point B)
\end{itemize}
  
\end{frame}

% ###################################################
\begin{frame}{Post DIGI filter working points}

For working point B ($p_\perp>40$, $|eta|<4.75$, $\Delta_{\eta}>3.0$, $\Delta_{\phi}<2.0$, $m_{jj}>1000$)

\begin{block}{Level 1 Cut}

  \centering
  \input{tbl/table_Events_EquiLumi10fb}
  
\end{block}

\begin{itemize}
  \item Sample would have around 80-200M events for 10 $fb^{-1}$ depending on L1 cut with this generator filter configuration
  \begin{itemize}
    \item What would be an acceptable upper limit to the samples size?
  \end{itemize}
  \item The preferred working point would be $L1_{ETM}\geq60$ or lower if possible.
\end{itemize}

\end{frame}

% ###################################################
\begin{frame}{Technical requirements}

If our proposal is accepted some technical requirements need to be fulfilled before central production can be started: 

\begin{block}{Requirements}
    
\begin{itemize}
  \item The generator dijet filter needs to be included in the CMSSW release.
  \begin{itemize} 
    \item Which version to target?
  \end{itemize}
  \item The post-DIGI filter needs to be included in the CMSSW release.
  \begin{itemize} 
    \item To what package?
  \end{itemize}
  \item cmsDriver needs to be updated to allow the inclusion of filters after DIGI.
\end{itemize}
    
\end{block}

\end{frame}

% ###################################################
\begin{frame}{Summary}
 
\begin{block}{Summary:}
 
\begin{itemize}
  \item A proposal of QCD samples with VBF jets and MET was presented.
  \item These samples would allow in depth study of QCD in the signal region of our analysis.
  \item To produce such samples some technical changes would be required.
  \item If produced this would be the first samples made with offline cuts in CMS.
  \item Would allow direct study of fake met due to miss measurement at high integrated luminosity for the first time.
\end{itemize}

\end{block}

\end{frame}

% ###################################################
\appendix
% ###################################################
\begin{frame}
 
\begin{block}

\begin{center}Backup Slides\end{center}

\end{block}

\end{frame}

% ###################################################
\begin{frame}{Run I QCD VBF Jets+MET samples}

As a reminder here are the generator cuts used to generate the run I QCD samples.

\begin{block}{MC Filter: Vectorial sum of neutrino $E_T$}

\begin{itemize}
  \item $\sum E_\perp(\vec{\nu}) > 40$ $GeV$
\end{itemize}

\end{block}

\begin{block}{MC Filter: Dijet Filter (AK5 GenJet no $\mu$)}

\begin{itemize}
  \item Select jets with:
  \begin{itemize}
    \item $p_\perp>20$ $GeV$
    \item $|\eta|<5.0$
  \end{itemize}
  \item From selected jets at least one pair with:
  \begin{itemize}
    \item $m_{jj}>700$ $GeV$
    \item $\Delta\eta>3.2$
  \end{itemize}    
\end{itemize}

\end{block}

\end{frame}

% ###################################################
\begin{frame}{Cross Sections}

\begin{block}{Cross Section for some QCD $p_\perp$ hats}
  
\input{tables/table_CrossSections_QCD.tex}

\end{block}
  
As expected cross section for the this QCD $p_\perp$ hats increase significantly from 8 to 13 TeV.
  
\end{frame}


% ###################################################
\begin{frame}{Unfiltered production: Hardware}

A single job for the whole simulation chain for 100 events was submitted to CERN lxbatch.
\begin{itemize}
  \item GEN, SIM, DIGI, L1, DIGI2RAW, HLT:GRun
  \item RAW2DIGI, L1Reco, RECO
\end{itemize}

\begin{columns}

  \column[t]{0.45\linewidth}  
  \centering
  
  \begin{block}{Hardware Step 1}
  \input{tbl/table_Step1_JobDetails}
  \end{block}
  
  \column[t]{0.45\linewidth}  
  \centering
  
  \begin{block}{Hardware Step 2}
  \input{tbl/table_Step1_JobDetails}
  \end{block}
  
\end{columns}


To note that at lxplus (which I will assume is representative of the grid resources) machines can be very different in terms of CPU and number of cores. Differences were observed in CPU time between machines executing exactly the same code of +50\% (sometimes as high as +100\%).

\end{frame}

% ###################################################
\begin{frame}{Unfiltered production: Computing times}
 
Here are the statistics for the computing

\begin{columns}

  \column[t]{0.45\linewidth}  
  \centering

  \begin{block}{Times Step 1}
  \input{tbl/table_Step1_TimeNorm}
  \end{block}
  
  \column[t]{0.45\linewidth}  
  \centering
  
  \begin{block}{Time Step 2}
  \input{tbl/table_Step2_TimeNorm}
  \end{block}

\end{columns}
  
Some conclusions
\begin{itemize}
  \item It would be impossible to process every single event, it would take several millennia on a single CPU. We need some kind of gen filter.
  \item Average processing times fluctuate a lot over different machines. We should use normalized time.
  \item On average events take between 1 and 5 mintutes to pass step one and a few seconds to pass second step.
\end{itemize}

NOTE: I am including PU at average 35 interactions and 50ns bunch separation.

\end{frame}

% ###################################################
\begin{frame}{Unfiltered production: Normalized Time}
 
I have decided to use normalized time (to some benchmark CPU power) for my calculation. Two units are normally used KSI2K and HS06 (default for the GRID).
 
\begin{columns}

  \column[t]{0.45\linewidth}  
  \centering

  \begin{block}{Times Step 1}
  \input{tbl/table_Step1_Time}
  \end{block}
  
  \column[t]{0.45\linewidth}  
  \centering
  
  \begin{block}{Time Step 2}
  \input{tbl/table_Step2_Time}
  \end{block}

\end{columns}

Some conclusions
\begin{itemize}
  \item Now normalized time scales up linearly with $p_\perp$ (as it should). 
  \item We will be using HS06 for our calculations
  \item The average factors (to convert back) for the step 1 jobs is $factor_{KSI2K}=1.5213$ and $factor_{HS06}=5.9330$
\end{itemize}


\end{frame}

% ###################################################
\begin{frame}{Generator level working points}

\begin{block}{Filter statistics}

  \centering
  \input{tbl/table_GenFilter_Summary}
  
\end{block}

\begin{itemize}
  \item Lowering $p_\perp$ even by 5 GeV increases processing time prohibitively 
  \item Lowering $\Delta\eta$ cut makes almost no differences
  \item For each 100 GeV drop of $m_{jj}$ will cost of at least 20 additional days. 
  \item Increasing $min(\Delta\phi)$ to 2.0 and decreasing $\Delta\eta$ to 3.0 is acceptable (Working point B)
\end{itemize}

\end{frame}

\end{document}
