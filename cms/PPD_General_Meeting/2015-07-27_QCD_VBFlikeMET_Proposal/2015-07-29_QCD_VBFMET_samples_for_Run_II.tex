\documentclass[8pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{epsfig}
% \usepackage{cancel}
\usepackage{ulem}
% \usepackage{threeparttable} % Joao Pela: 
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{siunitx}  % Allows easy x10^ for numbers
\usepackage{appendixnumberbeamer}

\usetheme{Madrid}

\author[J. Pela]{JoÃ£o Pela\\{\fontsize{5}{2}\selectfont on behalf of the Higgs to Invisible Analysis Group}}
\title{QCD VBF Jets+MET samples for Run 2}
\institute[ICL]{Imperial College London}
\date{2015-07-29}

% The log drawn in the upper right corner.
\logo{\includegraphics[height=0.115\paperheight]{img/Logo_CMSICL.png}}

\begin{document}
\setlength{\unitlength}{1mm}

% ###################################################
\begin{frame}
  \titlepage
\end{frame}

% Jim Points 
% - we have some problems with QCD MC resulting from our signal region : high mass jet pair with relatively modest pT and MET
% - our ability to study QCD BG estimation methods is hampered by lack of MC statistics
% - for parked analysis we generated a private QCD sample with GEN level filters, 
% - 
% - 
% - brief overview of the possible selections Joao has studied, and calculated the filter efficiencies
% - in order to make a cmsDriver command, we need changes to cmsDriver (to include post-L1 filter) and addition of the filter itself

% ###################################################
\begin{frame}{Introduction}
  
\begin{block}{Signal Topology: VBF Higgs to Invisible}
  
\begin{itemize}
  \item Two VBF jets with high mass but relatively low $p_\perp$ ($\sim 50$ GeV)
  \item Reasonable amount of MET
\end{itemize}
  
\end{block}

Our ability to study QCD BG estimation methods is hampered by lack of MC statistics

\begin{block}{QCD Private production during Run I}

During Run I a set of QCD samples with VBF like jets and real MET was generated with:

\begin{itemize}
  \item Real MET (sum os neutrino $p_\perp$)
  \item Two generator jets with high separation and mass
\end{itemize}

They allowed:

\begin{itemize}
  \item Understand real MET in QCD
  \item Indirectly ``determine the importance`` of fake MET events (which we were not modelling)
\end{itemize}
  
\end{block}

\begin{center}
This is not ideal because fake MET is important in our signal region
\end{center}

\end{frame}


% ###################################################
\begin{frame}{Proposal}

We propose to generate a sample of QCD with VBF jet selection at GEN level, MET applied after digitisation (which practically speaking means making a requirement at L1 trigger level)

\end{frame}

% ###################################################
\begin{frame}{Unfiltered production: Hardware}

A single job for the whole simulation chain for 100 events was submitted to CERN lxbatch.
\begin{itemize}
  \item GEN, SIM, DIGI, L1, DIGI2RAW, HLT:GRun
  \item RAW2DIGI, L1Reco, RECO
\end{itemize}

\begin{columns}

  \column[t]{0.45\linewidth}  
  \centering
  
  \begin{block}{Hardware Step 1}
  \input{tbl/table_Step1_JobDetails}
  \end{block}
  
  \column[t]{0.45\linewidth}  
  \centering
  
  \begin{block}{Hardware Step 2}
  \input{tbl/table_Step1_JobDetails}
  \end{block}
  
\end{columns}


To note that at lxplus (which I will assume is representative of the grid resources) machines can be very different in terms of CPU and number of cores. Differences were observed in CPU time between machines executing exactly the same code of +50\% (sometimes as high as +100\%).

\end{frame}

% ###################################################
\begin{frame}{Unfiltered production: Computing times}
 
Here are the statistics for the computing

\begin{columns}

  \column[t]{0.45\linewidth}  
  \centering

  \begin{block}{Times Step 1}
  \input{tbl/table_Step1_TimeNorm}
  \end{block}
  
  \column[t]{0.45\linewidth}  
  \centering
  
  \begin{block}{Time Step 2}
  \input{tbl/table_Step2_TimeNorm}
  \end{block}

\end{columns}
  
Some conclusions
\begin{itemize}
  \item It would be impossible to process every single event, it would take several millennia on a single CPU. We need some kind of gen filter.
  \item Average processing times fluctuate a lot over different machines. We should use normalized time.
  \item On average events take between 1 and 5 mintutes to pass step one and a few seconds to pass second step.
\end{itemize}

NOTE: I am including PU at average 35 interactions and 50ns bunch separation.

\end{frame}

% ###################################################
\begin{frame}{Unfiltered production: Normalized Time}
 
I have decided to use normalized time (to some benchmark CPU power) for my calculation. Two units are normally used KSI2K and HS06 (default for the GRID).
 
\begin{columns}

  \column[t]{0.45\linewidth}  
  \centering

  \begin{block}{Times Step 1}
  \input{tbl/table_Step1_Time}
  \end{block}
  
  \column[t]{0.45\linewidth}  
  \centering
  
  \begin{block}{Time Step 2}
  \input{tbl/table_Step2_Time}
  \end{block}

\end{columns}

Some conclusions
\begin{itemize}
  \item Now normalized time scales up linearly with $p_\perp$ (as it should). 
  \item We will be using HS06 for our calculations
  \item The average factors (to convert back) for the step 1 jobs is $factor_{KSI2K}=1.5213$ and $factor_{HS06}=5.9330$
\end{itemize}


\end{frame}

% ###################################################
\begin{frame}{Generator level working points}

\begin{block}{Filter statistics}

  \centering
  \input{tbl/table_GenFilter_Summary}
  
\end{block}

\begin{itemize}
  \item Lowering $p_\perp$ even by 5 GeV increases processing time prohibitively 
  \item Lowering $\Delta\eta$ cut makes almost no differences
  \item For each 100 GeV drop of $m_{jj}$ will cost of at least 20 additional days. 
  \item Increasing $min(\Delta\phi)$ to 2.0 and decreasing $\Delta\eta$ to 3.0 is acceptable (Working point B)
\end{itemize}

\end{frame}

% ###################################################
\begin{frame}{Offline Filter Efficiency}

For working point $p_\perp>40$, $|eta|<4.75$, $\Delta_{\eta}>3.0$, $\Delta_{\phi}<2.0$, $m_{jj}>1000$ 

\begin{block}{Level 1 Cut}

  \centering
  \input{tbl/table_Events_EquiLumi10fb}
  
\end{block}

\begin{itemize}
  \item Bug on $p_\perp>40$, $|eta|<4.75$ currently re-running (preliminary processing sees 10x less events in 30-50 bin)
  \item Sample would have around 115-170M events depending on L1 cut with this generator filter configuration
\end{itemize}

\end{frame}

% ###################################################
\begin{frame}{Summary}
 
\begin{block}{Summary:}
 
\begin{itemize}
  \item Found a working point with is feasible with no MET cut at generator level with the caveat that it has a delta phi cut.
  \item A document including all the information is being written and will be sent around soon.
\end{itemize}

\end{block}

\begin{block}{Next steps:}
 
\begin{itemize}
  \item Finish offline efficiency study
\end{itemize}

\end{block}

\end{frame}

% ###################################################
\appendix
% ###################################################
\begin{frame}{Run I QCD VBF Jets+MET samples}

As a reminder here are the generator cuts used to generate the run I QCD samples.

\begin{block}{MC Filter: Vectorial sum of neutrino $E_T$}

\begin{itemize}
  \item $\sum E_\perp(\vec{\nu}) > 40$ $GeV$
\end{itemize}

\end{block}

\begin{block}{MC Filter: Dijet Filter (AK5 GenJet no $\mu$)}

\begin{itemize}
  \item Select jets with:
  \begin{itemize}
    \item $p_\perp>20$ $GeV$
    \item $|\eta|<5.0$
  \end{itemize}
  \item From selected jets at least one pair with:
  \begin{itemize}
    \item $m_{jj}>700$ $GeV$
    \item $\Delta\eta>3.2$
  \end{itemize}    
\end{itemize}

\end{block}

\end{frame}

% ###################################################
\begin{frame}{Cross Sections}

\begin{block}{Cross Section for some QCD $p_\perp$ hats}
  
\input{tables/table_CrossSections_QCD.tex}

\end{block}
  
As expected cross section for the this QCD $p_\perp$ hats increase significantly from 8 to 13 TeV.
  
\end{frame}

% ###################################################
\begin{frame}{Events for 10 and 30 $fb^-1$}

\begin{block}

\input{tables/table_QCD_RunII_EventsIntegratedLuminsity.tex}

\end{block}
  
Knowing that the current QCD samples sizes (470-600: 2.9M and 600-800: 2.8M) we can conclude:
\begin{itemize}
  \item For 10 $fb^-1$ we need to simulate up to bin 470-600
  \item For 30 $fb^-1$ we need to simulate up to bin 600-800
\end{itemize}

\end{frame}

\end{document}
